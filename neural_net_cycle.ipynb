{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB/meucnvpM7GSQ0eKVknL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyberMonk999/Singe-Neuron-Learning-Life-cycle/blob/main/neural_net_cycle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Single-Cycle Learning Demonstration in Neural nets**\n",
        "\n",
        "\n",
        "This project provides a step-by-step, computational demonstration of the single learning cycle within a minimal neural network. The primary goal is to demystify the core mechanism of Deep Learning by showing how a network transitions from an initial, random guess to a measurably improved state. The methodology involves a complete trace of a single forward pass, the calculation of Mean Squared Error (MSE) Loss, and the subsequent corrective step using Backpropagation and Gradient Descent."
      ],
      "metadata": {
        "id": "GuI3sDQbYH8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We start by importing the libraries and functions.\n",
        "\n",
        " Numpy library provides efficient multi dimensional array objects and matrix multiplication.\n",
        "\n",
        " Sigmoid function is a non linear squishing function ythat allows the network to learn patterns by keeping the value of output between 0 and 1."
      ],
      "metadata": {
        "id": "UmoiIhKGDHBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#The sigmoid activation function maps any value to a range between 0 and 1\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "ct3YNmpPDOD6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now we set up the initial parametrs for a single learning cycle"
      ],
      "metadata": {
        "id": "VEQkuPQEEPJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input vector X which are 3 features of one sample\n",
        "\n",
        "X = np.array([[1.0, 0.5, 0.2]])\n",
        "\n",
        "#Initial Weights (W_old), here we choose a 3x2 matrix of random weights.\n",
        "\n",
        "# Row 1-3 corresponds to X's features; Column 1-2 corresponds to output neurons.\n",
        "W_initial = np.array([[0.1, 0.5],\n",
        "                  [0.9, 0.2],\n",
        "                  [-0.3, 0.7]])\n",
        "\n",
        "#Target Output (Y) shows What the network should produce.Here we want neuron 1 to be not activated but neuron 2 activated.\n",
        "\n",
        "# Neuron 1=0.0 (silent), Neuron 2=1.0 (firing).\n",
        "\n",
        "Y = np.array([[0.0, 1.0]])\n",
        "\n",
        "print(\"Initial Weights (W_intial) is: \\n \\n\", W_initial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bc-wJJlEWu1",
        "outputId": "a249115e-2de8-4479-d33d-d741030d4431"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights (W_intial) is: \n",
            " \n",
            " [[ 0.1  0.5]\n",
            " [ 0.9  0.2]\n",
            " [-0.3  0.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. After initializing the input, target and initial weights, the next step is where the input data travels through the network. This process is called forward pass and is the Linear combination of Input and Weights (Z = X @ W)"
      ],
      "metadata": {
        "id": "Tx7_lXFdGhvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Activation (Z) is the Linear combination of Input and Weights (Z = X @ W)\n",
        "# This is the sum of weighted inputs before the Sigmoid squashing\n",
        "\n",
        "Z = X @ W_initial\n",
        "\n",
        "print(\"Pre-Activation (Z) is: \\n \\n\", Z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwj353evHERs",
        "outputId": "ca7b65b5-6c1f-485a-caac-d6b6263edc07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Activation (Z) is: \n",
            " \n",
            " [[0.49 0.74]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we calculate the activation which is the networks final guess. The preactivation Z = X @ W_old sums up all the weighted inputs for each neuron.\n",
        "\n",
        "Preactivation function is the weighted sum of inputs before any activation function is applied while activation function takes this preactivation value as input and introduces non linearity. Activation function is applied on pre activation function to capture meaningful probabilistic realataionships by using mathematical functions like Sigmoid, ReLU etc.\n",
        "\n",
        " Preactivation is the input to decision maker and activation function is the decision maker itself turning the raw, unbounded potential (Z) into the network's probability-like output (A), giving us our initial guess about the output in the target."
      ],
      "metadata": {
        "id": "hcaLaGSSHaQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = sigmoid(Z)\n",
        "print(\"The activation function is : \\n\", A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugTXl38uHedB",
        "outputId": "64319ba8-3d42-4cc6-9b0c-12c30f79fb46"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The activation function is : \n",
            " [[0.62010643 0.67699586]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Once activation function took inital guess, the next step is to check how wrong the guess was compared to the desired target. This is decided by calculating th loss.\n",
        "\n",
        "Error is the raw difference between the target and the guess (Y - A). The loss os calclulated using Mean Squared Error method\n",
        "\n",
        "Note - np.square(Error): Squaring the error ensures that positive and negative mistakes are treated equally, and it severely penalizes larger errors, which is key for gradient descent."
      ],
      "metadata": {
        "id": "wdon9k5eKcek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Loss Calculation (The Mistake Score) ---\n",
        "\n",
        "# 1. Error: The raw difference between the target and the guess (Y - A)\n",
        "Error = Y - A\n",
        "\n",
        "# 2. Loss (Mean Squared Error - MSE): Sum of squared errors, divided by 2 neurons\n",
        "Loss_initial = np.sum(np.square(Error)) / 2\n",
        "\n",
        "print(\"Initial Error:\\n\", Error)\n",
        "print(\"Initial Loss (MSE):\\n\", Loss_initial)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZOvKSSDLa_B",
        "outputId": "57d4fdbb-97be-439c-c2fa-25e9e259fd5c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Error:\n",
            " [[-0.62010643  0.32300414]]\n",
            "Initial Loss (MSE):\n",
            " 0.24443183216018016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. As we calculated above the intial loss calulated is 0.2443. Our goal is to adjust the weights so that loss descends and reaches minimal (the least point it can take). Back propogation is the process which enables this.\n",
        "\n",
        "Backpropagation ($\\mathbf{dLoss/dW}$) Calculates the Gradient. The Gradient is a detailed map telling you, for every single weight, which direction (positive or negative) to move it to most quickly reduce the Loss. Think about it ike you are blindfolded on a hill. You stick your hand out and feel the ground to find the steepest downhill slope.\n",
        "\n",
        "Backpropogation uses Gradient descend and learning rate for finding this least point where loss is minimal.\n",
        "\n",
        ">>The Gradient is a detailed map telling you, for every single weight, which direction (positive or negative) to move it to most quickly reduce the Loss.\n",
        "\n",
        ">>Learning rate is the rate at which gradient decsend updates the model weight to find the minimum loss function.\n",
        "\n",
        "d_sigmoid_dz = A * (1 - A): This calculates the slope of the Sigmoid curve at the neuron's current output, which determines how sensitive the output is to a change in the weights.\n",
        "\n",
        "Delta = (A - Y) * d_sigmoid_dz: The Delta is the final, adjusted error signal found by multiplying the raw mistake by the slope, giving the neuron its appropriate \"blame score.\"\n",
        "\n",
        "d_loss_d_w = X.T @ Delta: The Gradient ($\\mathbf{dLoss/dW}$) is calculated by distributing the neuron's Delta (blame) back to all connecting weights, using the strength of the input ($\\mathbf{X}$) for each weight as the multiplier.\n"
      ],
      "metadata": {
        "id": "wHOboYbxMuns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: CALCULATE ERROR SIGNAL (DELTA) ---\n",
        "\n",
        "# 1a. Derivative of Sigmoid (derviative of A wrt Z): A * (1-A)\n",
        "# This calculates the slope of the activation function at the output point.\n",
        "d_sigmoid_dz = A * (1 - A)\n",
        "\n",
        "# 1b. The overall error signal (Delta):\n",
        "# Delta = (Output - Target) * Slope\n",
        "# This determines how much responsibility each output neuron has for the loss.\n",
        "Delta = (A - Y) * d_sigmoid_dz\n",
        "\n",
        "# Note: We use (A - Y) because the conventional derivative of MSE is (A - Y).\n",
        "\n",
        "print(\"Delta (Error Signal):\\n\", Delta)\n",
        "\n",
        "\n",
        "# --- STEP 2: CALCULATE THE GRADIENT (dLoss/dW) ---\n",
        "\n",
        "# The Chain Rule says: dLoss/dW = Input.T @ Delta\n",
        "# This maps the responsibility (Delta) back across the input lines (X) to every weight.\n",
        "d_loss_d_w = X.T @ Delta\n",
        "\n",
        "# Verification: The calculated value should match the constant we were using.\n",
        "print(\"Calculated Gradient (dLoss/dW):\\n\", d_loss_d_w)\n",
        "\n",
        "\n",
        "# --- STEP 3: APPLY GRADIENT DESCENT ---\n",
        "\n",
        "# Define a SAFE Learning Rate (LR)\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# 1. Calculate Adjustment: This is the step size we take in the direction of the gradient.\n",
        "# Adjustment = LR * Gradient\n",
        "Adjustment = LEARNING_RATE * d_loss_d_w\n",
        "\n",
        "# 2. Calculate New Weights: W_new = W_initial - Adjustment\n",
        "W_new = W_initial - Adjustment\n",
        "\n",
        "\n",
        "print(\"New Weights (W_new):\\n\", W_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w174Mpb8Pf4Z",
        "outputId": "942fedf9-a85e-4bf1-a7ed-5c3f34294c38"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delta (Error Signal):\n",
            " [[ 0.14608123 -0.07063211]]\n",
            "Calculated Gradient (dLoss/dW):\n",
            " [[ 0.14608123 -0.07063211]\n",
            " [ 0.07304061 -0.03531606]\n",
            " [ 0.02921625 -0.01412642]]\n",
            "New Weights (W_new):\n",
            " [[ 0.09985392  0.50007063]\n",
            " [ 0.89992696  0.20003532]\n",
            " [-0.30002922  0.70001413]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Gradient descend finds the specific amount of blame (and the required corrective action) for every single weight in the network, not just the one that is \"most to blame.\"The $\\mathbf{dLoss/dW}$ matrix we calculated (a 3x2 matrix) gives an individual adjustment value for all six weights simultaneously.This adjustment is proportional, means if one weight contributed 10 times more to the mistake than another, its value in the $\\mathbf{dLoss/dW}$ matrix will be 10 times larger, meaning it gets changed 10 times more.\n",
        "\n",
        "Sometimes, the Gradient value is negative, and since we subtract the adjustment (1$\\mathbf{W}_{\\text{new}} = \\mathbf{W}_{\\text{old}} - \\text{Adjustment}$), the weight value increases. The gradient descend process finds the optimal direction (increase or decrease) for each weight based on the calculus.If the weight needs to increase to lower the Loss, the Gradient value will be negativeIf the weight needs to decrease to lower the Loss, the Gradient value will be positive."
      ],
      "metadata": {
        "id": "GAX5vs6VTnky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 Once we get the adjusted weight, we do the forward pass again, with the new adjusted weight. The loss is then calculated to compared with old loss."
      ],
      "metadata": {
        "id": "NGrsvFfaU9J_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. New Pre-Activation (Z_new): Use the corrected weights\n",
        "Z_new = X @ W_new\n",
        "\n",
        "# 2. New Activation (A_new): The network's new guess\n",
        "A_new = sigmoid(Z_new)\n",
        "\n",
        "# 3. New Loss: Calculate the new mistake score\n",
        "Loss_new = np.sum(np.square(Y - A_new)) / 2\n",
        "\n",
        "print(\"\\n--- LEARNING CONFIRMATION ---\")\n",
        "print(\"New Activated Output (A_new):\\n\", A_new)\n",
        "print(\"New Loss:\", Loss_new)\n",
        "\n",
        "# Final check: Did the Loss decrease?\n",
        "if Loss_new < Loss_initial:\n",
        "    print(\"\\nSUCCESS! The Loss decreased. Learning confirmed.\")\n",
        "else:\n",
        "    print(\"\\nFAILURE! The Loss increased or stayed the same. Learning Rate was still too high.\")\n",
        "\n",
        "print(f\"Loss Delta: {Loss_initial - Loss_new}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYxkMHk4VaUV",
        "outputId": "8f56e43b-05c1-47a6-a9cc-a0b773929854"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- LEARNING CONFIRMATION ---\n",
            "New Activated Output (A_new):\n",
            " [[0.62006204 0.67701578]]\n",
            "New Loss: 0.24439786890413875\n",
            "\n",
            "SUCCESS! The Loss decreased. Learning confirmed.\n",
            "Loss Delta: 3.396325604140826e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion:\n",
        "\n",
        "Our project successfully documented the most fundamental unit of intelligence in a neural network, a single cycle of forward pass, loss quantification, and corrective backpropagation. By calculating the unique Gradient ($\\mathbf{dLoss/dW}$) and carefully tuning the Learning Rate to $0.001$, we demonstrated that the network can precisely identify and begin to correct its mistake.\n",
        "\n",
        "The simple $3 \\times 2$ neuron network we modeled becomes dramatically different when tackling big, real-world tasks like classifying millions of images or translating languages.\n",
        "\n",
        "**Specialized Loss Functions:** For classification tasks (like identifying cats/dogs), the Mean Squared Error (MSE) we used is replaced by Cross-Entropy Loss, which is better suited for probability distributions.\n",
        "\n",
        "**Adaptive Optimizers**: Instead of a fixed Learning Rate that applies to all weights, complex algorithms like Adam or RMSProp are used. These adaptive optimizers automatically calculate a different, customized learning rate for every single weight during every iteration, dramatically speeding up convergence and improving reliability.\n",
        "\n",
        "**Regularization**: To prevent the network from Overfitting (memorizing the training data instead of generalizing), techniques like Dropout (randomly turning off neurons during training) and weight decay are added to the learning cycle to force the network to be more robust.\n",
        "\n",
        "Input Layer: A vector of features (1$\\mathbf{X}$).\n",
        "\n",
        "Weights: Parameters (3$\\mathbf{W}$) that connect the inputs to the next layer.\n",
        "\n",
        "Non-linear Activation: The Sigmoid function, which transforms the simple weighted sum into a more complex, squashed output.\n",
        "\n",
        "Learning Mechanism: It used Backpropagation and Gradient Descent to adjust the weights based on a calculated loss."
      ],
      "metadata": {
        "id": "VGq1ABPMXRSs"
      }
    }
  ]
}